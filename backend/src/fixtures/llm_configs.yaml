# LLM Configurations fixture
# Seeds default LLM configurations for fresh installations
#
# Providers:
#   - ollama: Ollama server (HTTP API)
#   - local-llama-cpp: llama.cpp native (in-process, GGUF models)
#   - anthropic: Anthropic Claude API
#
# Default active model: Qwen3-coder:30b (Ollama)

# =============================================================================
# Ollama Models (localhost:11434)
# =============================================================================

- id: 1
  name: "Qwen3 Coder 30B (Default)"
  provider: "ollama"
  endpoint_url: "http://localhost:11434"
  model_name: "qwen3-coder:30b"
  temperature: 0.7
  max_tokens: 8192
  is_active: true

- id: 2
  name: "DeepSeek Coder V2 16B"
  provider: "ollama"
  endpoint_url: "http://localhost:11434"
  model_name: "deepseek-coder-v2:16b"
  temperature: 0.7
  max_tokens: 8192
  is_active: false

- id: 3
  name: "Codestral 22B"
  provider: "ollama"
  endpoint_url: "http://localhost:11434"
  model_name: "codestral:22b"
  temperature: 0.7
  max_tokens: 8192
  is_active: false

# =============================================================================
# Local llama.cpp Models (In-Process, GGUF)
# =============================================================================

- id: 4
  name: "MiniMax M2 Q5_K_M"
  provider: "local-llama-cpp"
  model_name: "MiniMax-M2-Q5_K_M"
  model_path: "llm-models/MiniMax-M2-Q5_K_M.gguf"
  n_ctx: 8192
  n_threads: 8
  temperature: 0.7
  max_tokens: 4096
  is_active: false

# =============================================================================
# Anthropic Models (Cloud API)
# =============================================================================

- id: 5
  name: "Claude Opus 4.5"
  provider: "anthropic"
  endpoint_url: "https://api.anthropic.com/v1"
  model_name: "claude-opus-4-5-20251101"
  # api_key: Set via environment variable or Admin UI
  temperature: 0.7
  max_tokens: 8192
  is_active: false
