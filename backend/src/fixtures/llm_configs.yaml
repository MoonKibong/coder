# LLM Configurations fixture
# Available models are fetched from the Ollama server

- id: 1
  name: "Qwen3 Coder 30B"
  provider: "ollama"
  endpoint_url: "http://localhost:11434"
  model_name: "qwen3-coder:30b"
  temperature: 0.7
  max_tokens: 8192
  is_active: true

- id: 2
  name: "DeepSeek Coder V2 16B"
  provider: "ollama"
  endpoint_url: "http://localhost:11434"
  model_name: "deepseek-coder-v2:16b"
  temperature: 0.7
  max_tokens: 8192
  is_active: true

- id: 3
  name: "Llama 3.1 8B"
  provider: "ollama"
  endpoint_url: "http://localhost:11434"
  model_name: "llama3.1:latest"
  temperature: 0.7
  max_tokens: 4096
  is_active: false

- id: 4
  name: "DeepSeek R1"
  provider: "ollama"
  endpoint_url: "http://localhost:11434"
  model_name: "deepseek-r1:latest"
  temperature: 0.7
  max_tokens: 4096
  is_active: false
