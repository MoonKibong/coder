<!-- Create LLM Config Modal -->
<div class="fixed inset-0 z-[60] bg-black/50">
    <div class="fixed inset-y-0 right-0 w-full max-w-2xl bg-background shadow-xl overflow-hidden flex flex-col">
        <!-- Header -->
        <div class="flex items-center justify-between px-6 py-4 border-b">
            <h2 class="text-lg font-semibold">New LLM Configuration</h2>
            <button onclick="document.getElementById('modal-container').innerHTML = ''"
                class="inline-flex items-center justify-center rounded-md h-8 w-8 hover:bg-accent">
                <svg class="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
                </svg>
            </button>
        </div>

        <!-- Form -->
        <form hx-post="/admin/llm-configs" hx-ext="json-enc"
              hx-target="#llm-config-tbody" hx-swap="afterbegin"
              hx-on::after-request="if(event.detail.successful) document.getElementById('modal-container').innerHTML = ''"
              class="flex-1 overflow-y-auto p-6">
            <div class="space-y-6">
                <!-- Name -->
                <div class="space-y-2">
                    <label for="name" class="text-sm font-medium">Name</label>
                    <input type="text" id="name" name="name" required
                        class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm
                               placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring"
                        placeholder="e.g., Production Ollama Server" />
                </div>

                <!-- Provider -->
                <div class="space-y-2">
                    <label for="provider" class="text-sm font-medium">Provider</label>
                    <select id="provider" name="provider" required
                        onchange="toggleLocalLlmFields()"
                        class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm
                               focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring">
                        <option value="">Select provider...</option>
                        <optgroup label="On-Premise (Production)">
                            <option value="ollama">Ollama</option>
                            <option value="llama-cpp">llama.cpp Server (HTTP)</option>
                            <option value="local-llama-cpp">llama.cpp Native (In-Process)</option>
                            <option value="vllm">vLLM</option>
                        </optgroup>
                        <optgroup label="Remote (Development)">
                            <option value="openai">OpenAI Compatible</option>
                            <option value="groq">Groq</option>
                            <option value="anthropic">Anthropic</option>
                        </optgroup>
                    </select>
                    <p class="text-xs text-muted-foreground">
                        For production, use on-premise providers. "llama.cpp Native" runs models in-process without a separate server.
                    </p>
                </div>

                <!-- Endpoint URL (hidden for local-llama-cpp) -->
                <div id="endpoint_url_section" class="space-y-2">
                    <label for="endpoint_url" class="text-sm font-medium">Endpoint URL</label>
                    <div class="flex gap-2">
                        <input type="url" id="endpoint_url" name="endpoint_url"
                            hx-get="/admin/llm-configs/models"
                            hx-trigger="change, keyup delay:500ms changed"
                            hx-target="#model_name_container"
                            hx-include="[name='endpoint_url']"
                            hx-indicator="#model-loading"
                            class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm font-mono
                                   placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring"
                            placeholder="http://localhost:11434" />
                        <span id="model-loading" class="htmx-indicator flex items-center">
                            <svg class="animate-spin h-5 w-5 text-muted-foreground" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                                <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                                <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                            </svg>
                        </span>
                    </div>
                    <p class="text-xs text-muted-foreground">
                        The LLM server endpoint. Models will be fetched automatically.
                    </p>
                </div>

                <!-- Local LLM Settings (only for local-llama-cpp) -->
                <div id="local_llm_section" class="space-y-4 p-4 rounded-lg border bg-blue-50 dark:bg-blue-950" style="display: none;">
                    <h3 class="text-sm font-medium text-blue-700 dark:text-blue-300">Local LLM Settings</h3>

                    <!-- Model Path -->
                    <div class="space-y-2">
                        <label for="model_path" class="text-sm font-medium">Model Path</label>
                        <input type="text" id="model_path" name="model_path"
                            class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm font-mono
                                   placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring"
                            placeholder="llm-models/your-model.gguf" />
                        <p class="text-xs text-muted-foreground">
                            Path to GGUF model file. Relative paths are from backend directory.
                        </p>
                    </div>

                    <div class="grid grid-cols-2 gap-4">
                        <!-- Context Size -->
                        <div class="space-y-2">
                            <label for="n_ctx" class="text-sm font-medium">Context Size</label>
                            <input type="number" id="n_ctx" name="n_ctx" value="4096" min="512" max="32768"
                                class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm
                                       focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring" />
                            <p class="text-xs text-muted-foreground">Context window size</p>
                        </div>

                        <!-- CPU Threads -->
                        <div class="space-y-2">
                            <label for="n_threads" class="text-sm font-medium">CPU Threads</label>
                            <input type="number" id="n_threads" name="n_threads" value="4" min="1" max="64"
                                class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm
                                       focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring" />
                            <p class="text-xs text-muted-foreground">Number of CPU threads</p>
                        </div>
                    </div>
                </div>

                <script>
                function toggleLocalLlmFields() {
                    const provider = document.getElementById('provider').value;
                    const isLocalLlm = provider === 'local-llama-cpp';

                    document.getElementById('endpoint_url_section').style.display = isLocalLlm ? 'none' : 'block';
                    document.getElementById('local_llm_section').style.display = isLocalLlm ? 'block' : 'none';

                    // Make endpoint_url not required for local-llama-cpp
                    document.getElementById('endpoint_url').required = !isLocalLlm;
                }
                </script>

                <!-- Model Name -->
                <div class="space-y-2">
                    <label for="model_name" class="text-sm font-medium">Model Name</label>
                    <div id="model_name_container">
                        {% if available_models | length > 0 %}
                        <select id="model_name" name="model_name" required
                            class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm font-mono
                                   focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring">
                            <option value="">Select model...</option>
                            {% for model in available_models %}
                            <option value="{{ model.name }}">{{ model.name }}{% if model.details and model.details.parameter_size %} ({{ model.details.parameter_size }}){% endif %}</option>
                            {% endfor %}
                        </select>
                        <p class="text-xs text-muted-foreground mt-1">
                            Found {{ available_models | length }} model(s) on the server.
                        </p>
                        {% else %}
                        <input type="text" id="model_name" name="model_name" required
                            class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm font-mono
                                   placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring"
                            placeholder="e.g., llama3.1:latest, codellama:13b" />
                        <p class="text-xs text-muted-foreground mt-1">
                            Enter the endpoint URL above to fetch available models.
                        </p>
                        {% endif %}
                    </div>
                </div>

                <!-- API Key (Optional) -->
                <div class="space-y-2">
                    <label for="api_key" class="text-sm font-medium">API Key (Optional)</label>
                    <input type="password" id="api_key" name="api_key"
                        class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm font-mono
                               placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring"
                        placeholder="sk-..." />
                    <p class="text-xs text-muted-foreground">
                        API key if required by the provider.
                    </p>
                </div>

                <!-- Generation Parameters -->
                <div class="space-y-4 p-4 rounded-lg border bg-muted/30">
                    <h3 class="text-sm font-medium">Generation Parameters</h3>

                    <div class="grid grid-cols-2 gap-4">
                        <!-- Temperature -->
                        <div class="space-y-2">
                            <label for="temperature" class="text-sm font-medium">Temperature</label>
                            <input type="number" id="temperature" name="temperature" value="0.7" min="0" max="2" step="0.1"
                                class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm
                                       focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring" />
                        </div>

                        <!-- Max Tokens -->
                        <div class="space-y-2">
                            <label for="max_tokens" class="text-sm font-medium">Max Tokens</label>
                            <input type="number" id="max_tokens" name="max_tokens" value="4096" min="256" max="32768"
                                class="flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm
                                       focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring" />
                        </div>
                    </div>
                </div>

                <!-- Is Active -->
                <div class="flex items-center space-x-2">
                    <input type="checkbox" id="is_active" name="is_active" value="true"
                        class="h-4 w-4 rounded border-input" />
                    <label for="is_active" class="text-sm font-medium">Active</label>
                </div>
            </div>
        </form>

        <!-- Footer -->
        <div class="flex items-center justify-end gap-2 px-6 py-4 border-t bg-muted/30">
            <button onclick="document.getElementById('modal-container').innerHTML = ''"
                class="inline-flex items-center justify-center rounded-md text-sm font-medium h-9 px-4 py-2
                       border bg-background shadow-sm hover:bg-accent hover:text-accent-foreground">
                Cancel
            </button>
            <button type="submit"
                onclick="this.closest('.fixed').querySelector('form').requestSubmit()"
                class="inline-flex items-center justify-center rounded-md text-sm font-medium h-9 px-4 py-2
                       bg-primary text-primary-foreground shadow-sm hover:bg-primary/90">
                Create Configuration
            </button>
        </div>
    </div>
</div>
